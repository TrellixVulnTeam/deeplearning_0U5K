{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FluidAINet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoadData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting utils/fluid_loader.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile utils/fluid_loader.py\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# encoding: utf-8\n",
    "\n",
    "\"\"\"\n",
    "@author: changxin\n",
    "@software: PyCharm\n",
    "@file: fluid_loader.py\n",
    "@time: 2018/5/4 15:04\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import time\n",
    "import operator\n",
    "import os\n",
    "\n",
    "from nose.tools import assert_equal\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "BASE_DIR = '/data/datasets/simulation_data'\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'water/0')\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.mkdir(DATA_DIR)\n",
    "\n",
    "def get_data_files():\n",
    "    filenames = []\n",
    "    # TODO multi folders for different frames\n",
    "    allfiles = os.listdir(DATA_DIR)\n",
    "    for filename in allfiles:\n",
    "        if filename.endswith('.csv'):\n",
    "            filenames.append(filename)\n",
    "    return map(lambda x: os.path.join(DATA_DIR, x), filenames)\n",
    "\n",
    "def create_train_files(max_num):\n",
    "    TRAIN_FILES = []\n",
    "    files_map = get_data_files()\n",
    "    i = 0\n",
    "    max_count = 2\n",
    "    for item in files_map:\n",
    "        if i == max_count:\n",
    "            break\n",
    "        TRAIN_FILES.append(item)\n",
    "        i = i + 1\n",
    "\n",
    "    assert_equal(len(TRAIN_FILES), max_count)\n",
    "    return TRAIN_FILES\n",
    "\n",
    "def convert_str_float(frame_particles):\n",
    "    fps = pd.DataFrame(frame_particles[1:], columns=frame_particles[0])\n",
    "    fps = fps[fps.columns[:-1]]\n",
    "    for col in fps.columns:\n",
    "        #if col == 'isFluidSolid':\n",
    "        fps[col] = fps[col].astype(float)\n",
    "    return fps\n",
    "\n",
    "def laod_csv(filename):\n",
    "    frame_particles = np.loadtxt(\n",
    "            filename, dtype=np.str, delimiter=\",\")\n",
    "    return convert_str_float(frame_particles)\n",
    "\n",
    "def load_data_file(filename):\n",
    "    suffix = filename.split('/')[-1].split('.')[-1]\n",
    "    print(suffix)\n",
    "    if suffix == 'csv':\n",
    "        return laod_csv(filename)\n",
    "\n",
    "def load_data_label(filename):\n",
    "    particles = load_data_file(filename)\n",
    "    cols = particles.columns\n",
    "    data_cols = operator.add(list(cols[0:6]), list(cols[7:9]))\n",
    "    label_cols = cols[15:18]\n",
    "    data = particles[data_cols].values\n",
    "    label = particles[label_cols].values\n",
    "    return data, label\n",
    "\n",
    "def shuffle_data(data, labels):\n",
    "    \"\"\" Shuffle data and labels.\n",
    "        Input:\n",
    "          data: B,N,... numpy array\n",
    "          label: B,N... numpy array\n",
    "        Return:\n",
    "          shuffled data, label and shuffle indices\n",
    "    \"\"\"\n",
    "    idx = np.arange(labels.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    return data[idx, ...], labels[idx, ...], idx\n",
    "\n",
    "def concat_data_label(train_files, max_points, dimention_data, dimention_label):\n",
    "    \"\"\"\n",
    "    intercept max_points\n",
    "    \"\"\"\n",
    "    TRAIN_FILES = train_files\n",
    "    train_file_idxs = np.arange(0, len(TRAIN_FILES))\n",
    "    np.random.shuffle(train_file_idxs)\n",
    "    def get_array(shape):\n",
    "        return np.empty(shape=shape)\n",
    "    FRAMES_NUM = len(TRAIN_FILES)\n",
    "    MAX_POINTS = max_points\n",
    "    DIMENTION_DATA = dimention_data\n",
    "    DIMENTION_LABEL = dimention_label\n",
    "    data_shape = (FRAMES_NUM, MAX_POINTS, DIMENTION_DATA)\n",
    "    \"\"\"\n",
    "    Different from the classification task, our lable is for every particle, we record label with (frame, particle index \\\n",
    "    , three-dimentions accelaration)(BxNx3)\n",
    "    \"\"\"\n",
    "    label_shape = (FRAMES_NUM, MAX_POINTS, DIMENTION_LABEL)\n",
    "    current_data = get_array(data_shape)\n",
    "    current_label = get_array(label_shape)\n",
    "    start = time.clock()\n",
    "    for fn in range(len(TRAIN_FILES)):\n",
    "        current_data_single, current_label_single = load_data_label(TRAIN_FILES[train_file_idxs[fn]])\n",
    "        current_data[fn] = current_data_single.values[:MAX_POINTS, :]\n",
    "        current_label[fn]= current_label_single.values[:MAX_POINTS, :]\n",
    "    running = time.clock() - start\n",
    "    print(\"runtime: %s\" % str(running))\n",
    "    return current_data, current_label\n",
    "\n",
    "def concat_data_label_all(train_files, dimention_data, dimention_label):\n",
    "    \"\"\"\n",
    "    use all data in a file\n",
    "    \"\"\"\n",
    "    TRAIN_FILES = train_files\n",
    "    train_file_idxs = np.arange(0, len(TRAIN_FILES))\n",
    "    np.random.shuffle(train_file_idxs)\n",
    "\n",
    "    DIMENTION_DATA = dimention_data\n",
    "    DIMENTION_LABEL = dimention_label\n",
    "    \"\"\"\n",
    "    Different from the classification task, our lable is for every particle, we record label with (frame, particle index \\\n",
    "    , three-dimentions accelaration)(BxNx3)\n",
    "    \"\"\"\n",
    "    current_data = []\n",
    "    current_label = []\n",
    "    start = time.clock()\n",
    "    for fn in range(len(TRAIN_FILES)):\n",
    "        current_data_single, current_label_single = load_data_label(TRAIN_FILES[train_file_idxs[fn]])\n",
    "        current_data.append(current_data_single)\n",
    "        current_label.append(current_label_single)\n",
    "    running = time.clock() - start\n",
    "    print(\"runtime: %s\" % str(running))\n",
    "    return current_data, current_label\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    BATCH_SIZE = 2\n",
    "    TRAIN_FILES = create_train_files(2)\n",
    "    print(TRAIN_FILES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/data/datasets/simulation_data/water/0/particles_2753.csv', '/data/datasets/simulation_data/water/0/particles_2559.csv']\n"
     ]
    }
   ],
   "source": [
    "%run -i utils/fluid_loader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv\n",
      "csv\n",
      "runtime: 3.9962249999999955\n"
     ]
    }
   ],
   "source": [
    "curren_data, current_label = concat_data_label_all(TRAIN_FILES, 8, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "curren_data = np.asarray(curren_data)\n",
    "current_label = np.asarray(current_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/data/datasets/simulation_data/water/0/particles_2753.csv', '/data/datasets/simulation_data/water/0/particles_2559.csv']\n",
      "csv\n",
      "csv\n",
      "runtime: 3.912877999999999\n",
      "Fluid\n"
     ]
    }
   ],
   "source": [
    "%run -i utils/preprocess.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62000, 3)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voxel_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train.py\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
