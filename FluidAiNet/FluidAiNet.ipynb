{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FluidAINet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoadData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv\n",
      "csv\n",
      "runtime: 3.9962249999999955\n"
     ]
    }
   ],
   "source": [
    "curren_data, current_label = concat_data_label_all(TRAIN_FILES, 8, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "curren_data = np.asarray(curren_data)\n",
    "current_label = np.asarray(current_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/data/datasets/simulation_data/water/0/particles_2753.csv', '/data/datasets/simulation_data/water/0/particles_2559.csv']\n",
      "csv\n",
      "csv\n",
      "runtime: 3.912877999999999\n",
      "Fluid\n"
     ]
    }
   ],
   "source": [
    "%run -i utils/preprocess.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62000, 3)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voxel_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "get all files including screens and frames\n",
    "\"\"\"\n",
    "BASE_DIR = '/data/datasets/simulation_data'\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'water')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '0']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_frames(data_dir=DATA_DIR):\n",
    "    dirs = os.listdir(data_dir)\n",
    "    frames = []\n",
    "    for item in dirs:\n",
    "        screen_path = os.path.join(data_dir, item)\n",
    "        allfiles = os.listdir(screen_path)\n",
    "        frames.extend(map(lambda x:os.path.join(item, x), allfiles))\n",
    "    return map(lambda x:os.path.join(data_dir, x), frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FILES = get_all_frames(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv\n"
     ]
    }
   ],
   "source": [
    "for f in TRAIN_FILES:\n",
    "    particles = load_data_file(f)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = particles.columns\n",
    "index = particles[particles[cols[7]]==0].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4528"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# particles.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting FuildAINetTest.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile FuildAINetTest.py\n",
    "\n",
    "import utils.fluid_loader as fl\n",
    "BASE_DIR = '/data/datasets/simulation_data'\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'water')\n",
    "\n",
    "class TestFluid(object):\n",
    "    def __init__(self):\n",
    "        self.data_dir = DATA_DIR\n",
    "    def test_iter_data(self):\n",
    "        for item in fl.iterate_data(self.data_dir):\n",
    "            print(item)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    testf1 = TestFluid()\n",
    "    testf1.test_iter_data()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils.fluid_loader' from '/data/deeplearning/FluidAiNet/utils/fluid_loader.py'>"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fluid\n",
      "(62032, 3)\n",
      "2357\n"
     ]
    }
   ],
   "source": [
    "import utils.fluid_loader as fl\n",
    "import imp\n",
    "imp.reload(fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object iterate_data at 0x7f6a9a025888>"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fl.iterate_data(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting utils/preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile utils/preprocess.py\n",
    "import os\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "from nose.tools import assert_equal\n",
    "\n",
    "from config import cfg\n",
    "\n",
    "data_dir = 'velodyne'\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "screen size (-8, -8, -8) (8, 8, 12)\n",
    "spacing_r 0.1f # diameter \n",
    "amoothRadius_h  2*spacing_r # so i decide the voxel_size 4*spacing_r\n",
    "particle size - radius\n",
    "x,y,z(21,21,26) (-1, -1, -1) (1, 1, 1.5)\n",
    "theoreticlly, \n",
    "\"\"\"\n",
    "\n",
    "def fluid_process_pointcloud(point_cloud, fluid_identification,cls=cfg.DETECT_OBJ):\n",
    "    if cls == 'Fluid':\n",
    "        print('Fluid')\n",
    "        scene_size = np.array([16, 16, 20], dtype=np.float32)\n",
    "        voxel_size = np.array([0.4, 0.4, 0.4], dtype=np.float32)\n",
    "        grid_size = np.array([40, 40, 50], dtype=np.int64)\n",
    "        lidar_coord = np.array([8, 8, 8], dtype=np.float32)\n",
    "        max_point_number = 64\n",
    "        # return\n",
    "    # FIXME  ccx AWESOME\n",
    "    print(point_cloud)\n",
    "    shifted_coord = point_cloud[:, :3] + lidar_coord\n",
    "    voxel_index = np.floor(\n",
    "        shifted_coord[:, :] / voxel_size).astype(np.int)  # int lower than num\n",
    "\n",
    "    print(voxel_index.shape)\n",
    "    bound_x = np.logical_and(\n",
    "        voxel_index[:, 2] >= 0, voxel_index[:, 2] < grid_size[2])\n",
    "    bound_y = np.logical_and(\n",
    "        voxel_index[:, 1] >= 0, voxel_index[:, 1] < grid_size[1])\n",
    "    bound_z = np.logical_and(\n",
    "        voxel_index[:, 0] >= 0, voxel_index[:, 0] < grid_size[0])\n",
    "\n",
    "    bound_box = np.logical_and(np.logical_and(bound_x, bound_y), bound_z)\n",
    "\n",
    "    point_cloud = point_cloud[bound_box]\n",
    "    voxel_index = voxel_index[bound_box]\n",
    "\n",
    "    # [K, 3] coordinate buffer as described in the paper\n",
    "    coordinate_buffer = np.unique(voxel_index, axis=0)\n",
    "\n",
    "    K = len(coordinate_buffer)  # K record the number of voxels\n",
    "    T = max_point_number\n",
    "    print(K)\n",
    "    assert_equal(T, 64)\n",
    "    # [K, 1] store number of points in each voxel grid\n",
    "    number_buffer = np.zeros(shape=(K), dtype=np.int64)\n",
    "\n",
    "    # [K, T, 7] feature buffer as described in the paper\n",
    "    feature_buffer = np.zeros(shape=(K, T, cfg.VOXEL_POINT_FEATURE), dtype=np.float32) # position, velocity, isFluid, index, relative position\n",
    "\n",
    "    # build a reverse index for coordinate buffer\n",
    "    index_buffer = {}\n",
    "    for i in range(K):\n",
    "        # mark the voxel,the position tuple of voxel as list(index_buffer)'s key\n",
    "        index_buffer[tuple(coordinate_buffer[i])] = i\n",
    "\n",
    "    for voxel, point in zip(voxel_index, point_cloud):\n",
    "        index = index_buffer[tuple(voxel)]\n",
    "        number = number_buffer[index]\n",
    "        if number < T:\n",
    "            feature_buffer[index, number, :8] = point\n",
    "            number_buffer[index] += 1\n",
    "\n",
    "    feature_buffer[:, :, -3:] = feature_buffer[:, :, :3] - \\\n",
    "                                feature_buffer[:, :, :3].sum(axis=1, keepdims=True) / number_buffer.reshape(K, 1, 1)\n",
    "\n",
    "    voxel_dict = {'feature_buffer': feature_buffer,  # (K, T, 7)\n",
    "                  'coordinate_buffer': coordinate_buffer,  # (K, 3)\n",
    "                  'number_buffer': number_buffer}  # (K,)\n",
    "    return voxel_dict\n",
    "\n",
    "def process_pointcloud(point_cloud, cls=cfg.DETECT_OBJ):\n",
    "    # Input:\n",
    "    #   (N, 4)\n",
    "    # Output:\n",
    "    #   voxel_dict\n",
    "    if cls == 'Car':\n",
    "        scene_size = np.array([4, 80, 70.4], dtype=np.float32)\n",
    "        voxel_size = np.array([0.4, 0.2, 0.2], dtype=np.float32)\n",
    "        grid_size = np.array([10, 400, 352], dtype=np.int64)\n",
    "        lidar_coord = np.array([0, 40, 3], dtype=np.float32)\n",
    "        max_point_number = 35\n",
    "    elif cls == 'Fluid':\n",
    "        print('Fluid')\n",
    "        scene_size = np.array([16, 16, 20], dtype=np.float32)\n",
    "        voxel_size = np.array([0.4, 0.4, 0.4], dtype=np.float32)\n",
    "        grid_size = np.array([10, 400, 352], dtype=np.int64)\n",
    "        lidar_coord = np.array([8, 8, 8], dtype=np.float32)\n",
    "        max_point_number = 64\n",
    "    else:\n",
    "        scene_size = np.array([4, 40, 48], dtype=np.float32)\n",
    "        voxel_size = np.array([0.4, 0.2, 0.2], dtype=np.float32)\n",
    "        grid_size = np.array([10, 200, 240], dtype=np.int64)\n",
    "        lidar_coord = np.array([0, 20, 3], dtype=np.float32)# fix coordinate to positive(z,y,x),which is the max num of minus\n",
    "        max_point_number = 45\n",
    "\n",
    "        np.random.shuffle(point_cloud)\n",
    "    # FIXME  ccx AWESOME\n",
    "    shifted_coord = point_cloud[:, :3] + lidar_coord\n",
    "    # reverse the point cloud coordinate (X, Y, Z) -> (Z, Y, X)\n",
    "    voxel_index = np.floor(\n",
    "        shifted_coord[:, ::-1] / voxel_size).astype(np.int) # int lower than num\n",
    "\n",
    "    bound_x = np.logical_and(\n",
    "        voxel_index[:, 2] >= 0, voxel_index[:, 2] < grid_size[2])\n",
    "    bound_y = np.logical_and(\n",
    "        voxel_index[:, 1] >= 0, voxel_index[:, 1] < grid_size[1])\n",
    "    bound_z = np.logical_and(\n",
    "        voxel_index[:, 0] >= 0, voxel_index[:, 0] < grid_size[0])\n",
    "\n",
    "    bound_box = np.logical_and(np.logical_and(bound_x, bound_y), bound_z)\n",
    "\n",
    "    point_cloud = point_cloud[bound_box]\n",
    "    voxel_index = voxel_index[bound_box]\n",
    "\n",
    "    # [K, 3] coordinate buffer as described in the paper\n",
    "    coordinate_buffer = np.unique(voxel_index, axis=0)\n",
    "\n",
    "    K = len(coordinate_buffer) # K record the number of voxels\n",
    "    T = max_point_number # TODO ccx set T reasonable?\n",
    "\n",
    "    # [K, 1] store number of points in each voxel grid\n",
    "    number_buffer = np.zeros(shape=(K), dtype=np.int64)\n",
    "\n",
    "    # [K, T, 7] feature buffer as described in the paper\n",
    "    feature_buffer = np.zeros(shape=(K, T, 7), dtype=np.float32)\n",
    "\n",
    "    # build a reverse index for coordinate buffer\n",
    "    index_buffer = {}\n",
    "    for i in range(K):\n",
    "        # mark the voxel,the position tuple of voxel as list(index_buffer)'s key\n",
    "        index_buffer[tuple(coordinate_buffer[i])] = i\n",
    "\n",
    "    for voxel, point in zip(voxel_index, point_cloud):\n",
    "        index = index_buffer[tuple(voxel)]\n",
    "        number = number_buffer[index]\n",
    "        if number < T:\n",
    "            feature_buffer[index, number, :4] = point\n",
    "            number_buffer[index] += 1\n",
    "\n",
    "    feature_buffer[:, :, -3:] = feature_buffer[:, :, :3] - \\\n",
    "        feature_buffer[:, :, :3].sum(axis=1, keepdims=True)/number_buffer.reshape(K, 1, 1)\n",
    "\n",
    "    voxel_dict = {'feature_buffer': feature_buffer, # (K, T, 7)\n",
    "                  'coordinate_buffer': coordinate_buffer, # (K, 3)\n",
    "                  'number_buffer': number_buffer} # (K,)\n",
    "    return voxel_dict\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "#     import fluid_loader\n",
    "\n",
    "#     BATCH_SIZE = 2\n",
    "#     TRAIN_FILES = fluid_loader.create_train_files(BATCH_SIZE)\n",
    "#     print(TRAIN_FILES)\n",
    "#     pointcloud, _, _ = fluid_loader.concat_data_label_all(TRAIN_FILES, 8, 3)\n",
    "#     voxel_index = fluid_process_pointcloud(pointcloud[0],1)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i utils/preprocess.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/data/datasets/simulation_data/water/1/particles_665.csv', '/data/datasets/simulation_data/water/1/particles_1504.csv']\n",
      "csv\n",
      "Fluid\n",
      "[[-1.      -1.       1.05335 ..., -8.21249  0.       0.     ]\n",
      " [-0.9     -1.       1.05335 ..., -8.21249  0.       0.     ]\n",
      " [-0.8     -1.       1.05335 ..., -8.21249  0.       0.     ]\n",
      " ..., \n",
      " [ 3.8      4.       3.95    ...,  0.       1.       0.     ]\n",
      " [ 3.9      4.       3.95    ...,  0.       1.       0.     ]\n",
      " [ 4.       4.       3.95    ...,  0.       1.       0.     ]]\n",
      "(62032, 3)\n",
      "2357\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'EasyDict' object has no attribute 'VOXEL_POINT_FEATURE'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/data/deeplearning/FluidAiNet/utils/preprocess.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_FILES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpointcloud\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_FILES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mvoxel_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfluid_process_pointcloud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpointcloud\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data/deeplearning/FluidAiNet/utils/preprocess.py\u001b[0m in \u001b[0;36mfluid_process_pointcloud\u001b[0;34m(point_cloud, fluid_identification, cls)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# [K, T, 7] feature buffer as described in the paper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0mfeature_buffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# position, velocity, isFluid, index, relative position\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;31m# build a reverse index for coordinate buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'EasyDict' object has no attribute 'VOXEL_POINT_FEATURE'"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 2\n",
    "TRAIN_FILES = fl.create_train_files(BATCH_SIZE)\n",
    "print(TRAIN_FILES)\n",
    "pointcloud, _, _ = fl.load_data_label(TRAIN_FILES[0])\n",
    "voxel_index = fluid_process_pointcloud(pointcloud, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'config' from '/data/deeplearning/FluidAiNet/config.py'>"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp.reload(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1, 2, 3], [4, 5, 6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2],\n",
       "       [3, 4, 5]])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a -[1,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
