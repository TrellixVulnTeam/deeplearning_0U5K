# 深度学习
# 用于图像分割的卷积神经网络

使用深度学习进行目标检测最大的困难可能是生成一个长度可变的边框列表。诚如是？

卷积特征图将图片的所有信息编码到深度的维度上，同时保留着原始图片上目标物体的相对位置信息。可以通过变换的关系，找到原始图片对应特征图上的像素区域（很重要的一点是即使我们是在特征图上生成的锚点，这些锚点最终是要映射回原始图片的尺寸），如果只用到卷积和池化，那么最终特征图的维度与原始图片是呈比例的，**数学上，如果图片的尺寸是 w×h，那么特征图最终会缩小到尺寸为 w/r 和 h/r，其中 r 是次级采样率。如果我们在特征图上每个空间位置上都定义一个锚点，那么最终图片的锚点会相隔 r 个像素，在 VGG 中，r=16**。



**RCNN**

R-CNN的三个步骤：

1. 为边界框生成一组提案
2. 将边界框输入CNN进行特征提取，并通过svm判定边界框内图像的目标
3. 将边框代入线性回归模型，一旦目标完成分类，输出边框的更紧密坐标，（即在边界框中找到了目标，能否使边框适应真实目标的尺寸）

R-CNN的缺陷，性能很棒（预测结果），但是运行很慢，主要体现在两个方面：

1. 需要CNN针对图像的每个region proposal进行约2000次的前向传递；
2. 分别训练3个不同的模型，CNN生成图像特征，预测类别的分类器和收紧边界框的的回归模型。

**Fast RCNN**

为解决这一问题，R-CNN 的第一作者 Ross Girshick提出fast RCNN

1. ROI Pooling

   最初在RCNN中，每一个提案都要单独的前向卷积运算，那么2000个左右的提案总共需要进行2000次前向卷积运算，那么是否可以只对原始图像进行一次卷积运算，然后在不同的2000个区域提案中共享	，即**创建了图像的完整前向传递，并从获得的前向传递中提取每个兴趣区域的转换特征。**

   那么如何通过从CNN的特征映射选择相应的区域来获取每个区域的CNN特征？

2. 将所有模型并入一个网络

**Faster RCNN**

为解决区域提案的生成问题，Faster RCNN提出重复使用CNN提取的特征图以取代单独运行选择性搜索算法。region proposal network，得到包含目标的边界框（即寻找可能包含目标的区域），并将这些候选边界框传递给后续的分类器和收紧边界框的回归模型。

（第一个真正意义上的端到端训练，主要是指RPN和主干网络一起训练）

**Mask R-CNN**
扩展Faster RCNN以用于像素级分割

RoiAlign——**准确地将原始图像的相关区域映射到特征图上**



那么我们继续深入一些细节

RPN

如何生成anchors

```python
feature_map_shape (batch, height, width, depth)
all_anchors (num_anchors_per_points * feature_width * feature_height, 4)
output_stride spatial shape [(height - 1) / output_stride + 1, (width - 1) / output_stride + 1]
```

针对每个锚点，我们需要考虑，这个锚点包含目标吗以及如何调整锚点以更好的拟合目标。



**非极大抑制（Non-maximum suppression）**:

解决一个目标被多次检测的问题

![/images_md/sc.png](/images_md/sc.png)

总算弄清楚一个问题，RPN网络针对一次卷积生成的特征图（512 for VCG-16）进行卷积，每个滑动窗口都会得到一个512-d的向量，而这一层次的核参数维度应当是 （512，3，3，512），同时新一层特征图有多大，就有几个这样的512-d向量



**关于固定大小的问题：**

经典的卷积神经网络有一个问题是它只能接受固定大小的输入图像，这是因为**第一个全连接层**和**它之前的卷积层之间（最后一个卷积层）**的权重矩阵大小是固定的，而卷积层、全连接层本身对输入图像的大小并没有限制。而在做目标检测时，卷积网络面临的输入候选区域图像大小尺寸是不固定的。

如何解决这个问题？

如果在最后一个卷积层和第一个全连接层之间做一些处理，将不同大小的图像变为固定大小的全连接层输入就可以解决问题。SPPNet引入了Spatial Pyramid pooling层



**RoI Pooling**

根据预选框的位置坐标，在特征图中将相应区域池化为固定尺寸的特征图，以便在最后的回归和分类问题中使用全连接层。



**Mask RCNN的RoI Align**

降低池化像素偏差