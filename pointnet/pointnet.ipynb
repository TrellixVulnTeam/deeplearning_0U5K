{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/f1001/.pyenv/versions/2.7.14/envs/pointnet/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import math\n",
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import socket\n",
    "import importlib\n",
    "import os\n",
    "import sys\n",
    "# BASE_DIR = os.path.dirname(os.path.abspath('.'))\n",
    "BASE_DIR = os.path.abspath('.')\n",
    "sys.path.append(BASE_DIR)\n",
    "sys.path.append(os.path.join(BASE_DIR, 'models'))\n",
    "sys.path.append(os.path.join(BASE_DIR, 'utils'))\n",
    "import provider\n",
    "import tf_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_POINT = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ModelNet40 official train/test split\n",
    "TRAIN_FILES = provider.getDataFiles( \\\n",
    "    os.path.join(BASE_DIR, 'data/modelnet40_ply_hdf5_2048/train_files.txt'))\n",
    "TEST_FILES = provider.getDataFiles(\\\n",
    "    os.path.join(BASE_DIR, 'data/modelnet40_ply_hdf5_2048/test_files.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_data, current_label = provider.loadDataFile(TRAIN_FILES[train_file_idxs[0]])\n",
    "current_data = current_data[:,0:NUM_POINT,:]\n",
    "current_data, current_label, _ = provider.shuffle_data(current_data, np.squeeze(current_label))            \n",
    "current_label = np.squeeze(current_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2048, 1024, 3)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14, 36, 25, ...,  4, 17, 28], dtype=uint8)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle train files\n",
    "train_file_idxs = np.arange(0, len(TRAIN_FILES))\n",
    "np.random.shuffle(train_file_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "types = []\n",
    "for lable in current_label:\n",
    "    if lable not in types:\n",
    "        types.append(lable)\n",
    "len(types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 3, 2, 1, 4])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 4, 2, 3, 1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.shuffle(train_file_idxs)\n",
    "train_file_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(current_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda/include/cudnn.h\n",
      "cuda/lib64/libcudnn.so\n",
      "cuda/lib64/libcudnn.so.5\n",
      "cuda/lib64/libcudnn.so.5.1.10\n",
      "cuda/lib64/libcudnn_static.a\n"
     ]
    }
   ],
   "source": [
    "!tar -zxvf ~/Downloads/cudnn-8.0-linux-x64-v5.1.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset dataspace is (4, 6)\n",
      "Dataset Numpy datatype is >i4\n",
      "Dataset name is /dset\n",
      "Dataset is a member of the group <HDF5 group \"/\" (1 members)>\n",
      "Dataset was created in the file <HDF5 file \"dset.h5\" (mode r+)>\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# This examaple creates an HDF5 file dset.h5 and an empty datasets /dset in it.\n",
    "#\n",
    "import h5py\n",
    "#\n",
    "# Create a new file using defaut properties.\n",
    "#\n",
    "file = h5py.File('dset.h5','w')\n",
    "#\n",
    "# Create a dataset under the Root group.\n",
    "#\n",
    "dataset = file.create_dataset(\"dset\",(4, 6), h5py.h5t.STD_I32BE)\n",
    "print \"Dataset dataspace is\", dataset.shape\n",
    "print \"Dataset Numpy datatype is\", dataset.dtype\n",
    "print \"Dataset name is\", dataset.name\n",
    "print \"Dataset is a member of the group\", dataset.parent\n",
    "print \"Dataset was created in the file\", dataset.file\n",
    "#\n",
    "# Close the file before exiting\n",
    "#\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN FLUID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load_data.py content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting load_data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile load_data.py\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import operator\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import h5py\n",
    "from nose.tools import assert_equal\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "BASE_DIR = '/data/datasets/simulation_data'\n",
    "\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'water/0')\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.mkdir(DATA_DIR)\n",
    "\n",
    "def get_data_files():\n",
    "    filenames = []\n",
    "    # TODO multi folders for different frames\n",
    "    allfiles = os.listdir(DATA_DIR)\n",
    "    for filename in allfiles:\n",
    "        if filename.endswith('.csv'): \n",
    "            filenames.append(filename)\n",
    "    return map(lambda x: os.path.join(DATA_DIR, x), filenames)\n",
    "\n",
    "def convert_str_float(frame_particles):\n",
    "    fps = pd.DataFrame(frame_particles[1:], columns=frame_particles[0])\n",
    "    fps = fps[fps.columns[:-1]]\n",
    "    for col in fps.columns: \n",
    "        #if col == 'isFluidSolid':\n",
    "        fps[col] = fps[col].astype(float)\n",
    "    return fps\n",
    "\n",
    "def laod_csv(filename):\n",
    "    frame_particles = np.loadtxt(\n",
    "            filename, dtype=np.str, delimiter=\",\")\n",
    "    return convert_str_float(frame_particles)\n",
    "\n",
    "def load_data_file(filename):\n",
    "    return laod_csv(filename)\n",
    "\n",
    "def load_data_label(filename):\n",
    "    particles = laod_csv(filename)\n",
    "    cols = particles.columns\n",
    "    data_cols = operator.add(list(cols[0:6]), list(cols[7:9]))\n",
    "    label_cols = cols[15:18]\n",
    "    data = particles[data_cols]\n",
    "    label = particles[label_cols]\n",
    "    return data, label\n",
    "\n",
    "def shuffle_data(data, labels):\n",
    "    \"\"\" Shuffle data and labels.\n",
    "        Input:\n",
    "          data: B,N,... numpy array\n",
    "          label: B,N... numpy array\n",
    "        Return:\n",
    "          shuffled data, label and shuffle indices\n",
    "    \"\"\"\n",
    "    idx = np.arange(labels.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    return data[idx, ...], labels[idx, ...], idx\n",
    "\n",
    "# Shuffle train files\n",
    "def concat_data_label(train_files, max_points, dimention_data, dimention_label):\n",
    "    TRAIN_FILES = train_files\n",
    "    train_file_idxs = np.arange(0, len(TRAIN_FILES))\n",
    "    np.random.shuffle(train_file_idxs)\n",
    "    def get_array(shape):\n",
    "        return np.empty(shape=shape)\n",
    "    FRAMES_NUM = len(TRAIN_FILES)\n",
    "    MAX_POINTS = max_points\n",
    "    DIMENTION_DATA = dimention_data\n",
    "    DIMENTION_LABEL = dimention_label\n",
    "    data_shape = (FRAMES_NUM, MAX_POINTS, DIMENTION_DATA)\n",
    "    \"\"\"\n",
    "    Different from the classification task, our lable is for every particle, we record label with (frame, particle index \\\n",
    "    , three-dimentions accelaration)(BxNx3)\n",
    "    \"\"\"\n",
    "    label_shape = (FRAMES_NUM, MAX_POINTS, DIMENTION_LABEL)\n",
    "    current_data = get_array(data_shape)\n",
    "    current_label = get_array(label_shape)\n",
    "    start = time.clock()\n",
    "    for fn in range(len(TRAIN_FILES)):\n",
    "        current_data_single, current_label_single = load_data_label(TRAIN_FILES[train_file_idxs[fn]])\n",
    "        current_data[fn] = current_data_single.values[:MAX_POINTS, :]\n",
    "        current_label[fn]= current_label_single.values[:MAX_POINTS, :]\n",
    "    running = time.clock() - start\n",
    "    print \"runtime: %s\" % str(running)\n",
    "    return current_data, current_label\n",
    "\n",
    "def jitter_point_cloud(batch_data, sigma=0.01, clip=0.05):\n",
    "    \"\"\" Randomly jitter points. jittering is per point.\n",
    "        Input:\n",
    "          BxNx3 array, original batch of point clouds\n",
    "        Return:\n",
    "          BxNx3 array, jittered batch of point clouds\n",
    "    \"\"\"\n",
    "    B, N, C = batch_data.shape\n",
    "    assert(clip > 0)\n",
    "    jittered_data = np.clip(sigma * np.random.randn(B, N, C), -1*clip, clip)\n",
    "    jittered_data += batch_data\n",
    "    return jittered_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### load_data.py test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "run -i load_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "TRAIN_FILES = list(get_data_files())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runtime: 15.80032\n"
     ]
    }
   ],
   "source": [
    "data, label = concat_data_label(TRAIN_FILES, 10000, 8, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10000, 3)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_s, label_s, index = shuffle_data(data, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "jitter_data = jitter_point_cloud(data_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train_fluid.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train_fluid.py\n",
    "\n",
    "import argparse\n",
    "import math\n",
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import socket\n",
    "import importlib\n",
    "import os\n",
    "import sys\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "sys.path.append(BASE_DIR)\n",
    "sys.path.append(os.path.join(BASE_DIR, 'models'))\n",
    "sys.path.append(os.path.join(BASE_DIR, 'utils'))\n",
    "# import provider\n",
    "import load_data\n",
    "import tf_util\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--gpu', type=int, default=0, help='GPU to use [default: GPU 0]')\n",
    "parser.add_argument('--model', default='pointnet_cls', help='Model name: pointnet_cls or pointnet_cls_basic [default: pointnet_cls]')\n",
    "parser.add_argument('--log_dir', default='log', help='Log dir [default: log]')\n",
    "parser.add_argument('--num_point', type=int, default=10000, help='Point Number [256/512/1024/2048] [default: 1024]')\n",
    "parser.add_argument('--max_epoch', type=int, default=250, help='Epoch to run [default: 250]')\n",
    "parser.add_argument('--batch_size', type=int, default=2, help='Batch Size during training [default: 32]')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.001, help='Initial learning rate [default: 0.001]')\n",
    "parser.add_argument('--momentum', type=float, default=0.9, help='Initial learning rate [default: 0.9]')\n",
    "parser.add_argument('--optimizer', default='adam', help='adam or momentum [default: adam]')\n",
    "parser.add_argument('--decay_step', type=int, default=200000, help='Decay step for lr decay [default: 200000]')\n",
    "parser.add_argument('--decay_rate', type=float, default=0.7, help='Decay rate for lr decay [default: 0.8]')\n",
    "FLAGS = parser.parse_args()\n",
    "\n",
    "\n",
    "BATCH_SIZE = FLAGS.batch_size\n",
    "NUM_POINT = FLAGS.num_point\n",
    "MAX_EPOCH = FLAGS.max_epoch\n",
    "BASE_LEARNING_RATE = FLAGS.learning_rate\n",
    "GPU_INDEX = FLAGS.gpu\n",
    "MOMENTUM = FLAGS.momentum #\n",
    "OPTIMIZER = FLAGS.optimizer\n",
    "DECAY_STEP = FLAGS.decay_step #\n",
    "DECAY_RATE = FLAGS.decay_rate #\n",
    "\n",
    "MODEL = importlib.import_module(FLAGS.model) # import network module\n",
    "MODEL_FILE = os.path.join(BASE_DIR, 'models', FLAGS.model+'.py')\n",
    "LOG_DIR = FLAGS.log_dir\n",
    "if not os.path.exists(LOG_DIR): os.mkdir(LOG_DIR)\n",
    "os.system('cp %s %s' % (MODEL_FILE, LOG_DIR)) # bkp of model def\n",
    "os.system('cp train.py %s' % (LOG_DIR)) # bkp of train procedure\n",
    "LOG_FOUT = open(os.path.join(LOG_DIR, 'log_train.txt'), 'w')\n",
    "LOG_FOUT.write(str(FLAGS)+'\\n')\n",
    "\n",
    "MAX_NUM_POINT = 2048 # TODO the count of particles\n",
    "NUM_CLASSES = 40 # the outputs of network\n",
    "\n",
    "BN_INIT_DECAY = 0.5\n",
    "BN_DECAY_DECAY_RATE = 0.5\n",
    "BN_DECAY_DECAY_STEP = float(DECAY_STEP)\n",
    "BN_DECAY_CLIP = 0.99\n",
    "\n",
    "HOSTNAME = socket.gethostname()\n",
    "\n",
    "# TODO load sample files\n",
    "TRAIN_FILES = load_data.get_data_files()[:5]\n",
    "TEST_FILES = load_data.get_data_files()[5:6]\n",
    "\n",
    "def log_string(out_str):\n",
    "    LOG_FOUT.write(out_str+'\\n')\n",
    "    LOG_FOUT.flush()\n",
    "    print(out_str)\n",
    "\n",
    "\n",
    "def get_learning_rate(batch):\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "                        BASE_LEARNING_RATE,  # Base learning rate.\n",
    "                        batch * BATCH_SIZE,  # Current index into the dataset.\n",
    "                        DECAY_STEP,          # Decay step.\n",
    "                        DECAY_RATE,          # Decay rate.\n",
    "                        staircase=True)\n",
    "    learning_rate = tf.maximum(learning_rate, 0.00001) # CLIP THE LEARNING RATE!\n",
    "    return learning_rate        \n",
    "\n",
    "def get_bn_decay(batch):\n",
    "    bn_momentum = tf.train.exponential_decay(\n",
    "                      BN_INIT_DECAY,\n",
    "                      batch*BATCH_SIZE,\n",
    "                      BN_DECAY_DECAY_STEP,\n",
    "                      BN_DECAY_DECAY_RATE,\n",
    "                      staircase=True)\n",
    "    bn_decay = tf.minimum(BN_DECAY_CLIP, 1 - bn_momentum)\n",
    "    return bn_decay\n",
    "\n",
    "def train():\n",
    "    with tf.Graph().as_default():\n",
    "        with tf.device('/gpu:'+str(GPU_INDEX)):\n",
    "            pointclouds_pl, labels_pl = MODEL.placeholder_inputs(BATCH_SIZE, NUM_POINT)\n",
    "            is_training_pl = tf.placeholder(tf.bool, shape=()) # TODO what is 'is_training_pl' for?\n",
    "            print(is_training_pl)\n",
    "            \n",
    "            # Note the global_step=batch parameter to minimize. \n",
    "            # That tells the optimizer to helpfully increment the 'batch' parameter for you every time it trains.\n",
    "            batch = tf.Variable(0)\n",
    "            bn_decay = get_bn_decay(batch)\n",
    "            tf.summary.scalar('bn_decay', bn_decay)\n",
    "\n",
    "            # Get model and loss \n",
    "            pred, end_points = MODEL.get_model(pointclouds_pl, is_training_pl, bn_decay=bn_decay)\n",
    "            loss = MODEL.get_loss(pred, labels_pl, end_points) # TODO change to regression loss\n",
    "            tf.summary.scalar('loss', loss)\n",
    "\n",
    "#             correct = tf.equal(tf.argmax(pred, 1), tf.to_int64(labels_pl))\n",
    "#             accuracy = tf.reduce_sum(tf.cast(correct, tf.float32)) / float(BATCH_SIZE)\n",
    "#             tf.summary.scalar('accuracy', accuracy)\n",
    "#             # end loss\n",
    "\n",
    "            # Get training operator\n",
    "            learning_rate = get_learning_rate(batch)\n",
    "            tf.summary.scalar('learning_rate', learning_rate)\n",
    "            if OPTIMIZER == 'momentum':\n",
    "                optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=MOMENTUM)\n",
    "            elif OPTIMIZER == 'adam':\n",
    "                optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            train_op = optimizer.minimize(loss, global_step=batch)\n",
    "            \n",
    "            # Add ops to save and restore all the variables.\n",
    "            saver = tf.train.Saver()\n",
    "            \n",
    "        # Create a session\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        config.allow_soft_placement = True\n",
    "        config.log_device_placement = False\n",
    "        sess = tf.Session(config=config)\n",
    "\n",
    "        # Add summary writers\n",
    "        #merged = tf.merge_all_summaries()\n",
    "        merged = tf.summary.merge_all()\n",
    "        train_writer = tf.summary.FileWriter(os.path.join(LOG_DIR, 'train'),\n",
    "                                  sess.graph)\n",
    "        test_writer = tf.summary.FileWriter(os.path.join(LOG_DIR, 'test'))\n",
    "\n",
    "        # Init variables\n",
    "        init = tf.global_variables_initializer()\n",
    "        # To fix the bug introduced in TF 0.12.1 as in\n",
    "        # http://stackoverflow.com/questions/41543774/invalidargumenterror-for-tensor-bool-tensorflow-0-12-1\n",
    "        #sess.run(init)\n",
    "        sess.run(init, {is_training_pl: True})\n",
    "\n",
    "        ops = {'pointclouds_pl': pointclouds_pl,\n",
    "               'labels_pl': labels_pl,\n",
    "               'is_training_pl': is_training_pl,\n",
    "               'pred': pred,\n",
    "               'loss': loss,\n",
    "               'train_op': train_op,\n",
    "               'merged': merged,\n",
    "               'step': batch}\n",
    "\n",
    "        for epoch in range(MAX_EPOCH): # same data with different order\n",
    "            log_string('**** EPOCH %03d ****' % (epoch))\n",
    "            sys.stdout.flush()\n",
    "             \n",
    "            train_one_epoch(sess, ops, train_writer)\n",
    "            eval_one_epoch(sess, ops, test_writer)\n",
    "            \n",
    "            # Save the variables to disk.\n",
    "            if epoch % 10 == 0:\n",
    "                save_path = saver.save(sess, os.path.join(LOG_DIR, \"model.ckpt\"))\n",
    "                log_string(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "\n",
    "\n",
    "def train_one_epoch(sess, ops, train_writer):\n",
    "    \"\"\" ops: dict mapping from string to tf ops \"\"\"\n",
    "    is_training = True\n",
    "    current_data, current_label = load_data.concat_data_label(TRAIN_FILES, NUM_POINT, 8, 3)\n",
    "    \n",
    "    file_size = current_data.shape[0]\n",
    "    num_batches = file_size // BATCH_SIZE\n",
    "    print('num_batches:', num_batches)\n",
    "    \n",
    "    total_correct = 0\n",
    "    total_seen = 0\n",
    "    loss_sum = 0\n",
    "       \n",
    "    for batch_idx in range(num_batches):\n",
    "        start_idx = batch_idx * BATCH_SIZE\n",
    "        end_idx = (batch_idx+1) * BATCH_SIZE\n",
    "\n",
    "        # Augment batched point clouds by rotation and jittering \n",
    "        # TODO AM I also need to do this?\n",
    "#         rotated_data = provider.rotate_point_cloud(current_data[start_idx:end_idx, :, :])\n",
    "        jittered_data = load_data.jitter_point_cloud(current_data[start_idx:end_idx, :, :])\n",
    "        feed_dict = {ops['pointclouds_pl']: jittered_data,\n",
    "                     ops['labels_pl']: current_label[start_idx:end_idx, :, :],\n",
    "                     ops['is_training_pl']: is_training,}\n",
    "        summary, step, _, loss_val, pred_val = sess.run([ops['merged'], ops['step'],\n",
    "            ops['train_op'], ops['loss'], ops['pred']], feed_dict=feed_dict)\n",
    "        train_writer.add_summary(summary, step)\n",
    "#         pred_val = np.argmax(pred_val, 1) # The second para for axis 0 for col and 1 for row ,return index\n",
    "#         correct = np.sum(pred_val == current_label[start_idx:end_idx])\n",
    "#         total_correct += correct\n",
    "#         total_seen += BATCH_SIZE\n",
    "        loss_sum += loss_val\n",
    "\n",
    "    log_string('mean loss: %f' % (loss_sum / float(num_batches)))\n",
    "#     log_string('accuracy: %f' % (total_correct / float(total_seen)))\n",
    "\n",
    "        \n",
    "def eval_one_epoch(sess, ops, test_writer):\n",
    "    \"\"\" ops: dict mapping from string to tf ops \"\"\"\n",
    "    is_training = False\n",
    "    total_correct = 0\n",
    "    total_seen = 0\n",
    "    loss_sum = 0\n",
    "#     total_seen_class = [0 for _ in range(NUM_CLASSES)]\n",
    "#     total_correct_class = [0 for _ in range(NUM_CLASSES)]\n",
    "    \n",
    "    MAX_POINTS = 10000\n",
    "    current_data, current_label = load_data.concat_data_label(TRAIN_FILES, MAX_POINTS, 8, 3)\n",
    "    \n",
    "    file_size = current_data.shape[0]\n",
    "    num_batches = file_size // BATCH_SIZE\n",
    "        \n",
    "    for batch_idx in range(num_batches):\n",
    "        start_idx = batch_idx * BATCH_SIZE\n",
    "        end_idx = (batch_idx+1) * BATCH_SIZE\n",
    "\n",
    "        feed_dict = {ops['pointclouds_pl']: current_data[start_idx:end_idx, :, :],\n",
    "                     ops['labels_pl']: current_label[start_idx:end_idx],\n",
    "                     ops['is_training_pl']: is_training}\n",
    "        summary, step, loss_val, pred_val = sess.run([ops['merged'], ops['step'],\n",
    "            ops['loss'], ops['pred']], feed_dict=feed_dict)\n",
    "#         pred_val = np.argmax(pred_val, 1)\n",
    "#         correct = np.sum(pred_val == current_label[start_idx:end_idx])\n",
    "#         total_correct += correct\n",
    "        total_seen += BATCH_SIZE\n",
    "        loss_sum += (loss_val*BATCH_SIZE)\n",
    "#         for i in range(start_idx, end_idx):\n",
    "#             l = current_label[i]\n",
    "#             total_seen_class[l] += 1\n",
    "#             total_correct_class[l] += (pred_val[i-start_idx] == l)\n",
    "            \n",
    "    log_string('eval mean loss: %f' % (loss_sum / float(total_seen)))\n",
    "#     log_string('eval accuracy: %f'% (total_correct / float(total_seen)))\n",
    "#     log_string('eval avg class acc: %f' % (np.mean(np.array(total_correct_class)/np.array(total_seen_class,dtype=np.float))))\n",
    "         \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n",
    "    LOG_FOUT.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add train_fluid.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master 73bd6d7] change format of training data\r\n",
      " 1 file changed, 254 insertions(+)\r\n",
      " create mode 100644 train_fluid.py\r\n"
     ]
    }
   ],
   "source": [
    "!git ci -m \"change format of training data \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pointnet_cls.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting models/pointnet_cls.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile models/pointnet_cls.py\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "import os\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "sys.path.append(BASE_DIR)\n",
    "sys.path.append(os.path.join(BASE_DIR, '../utils'))\n",
    "import tf_util\n",
    "from transform_nets import input_transform_net, feature_transform_net\n",
    "\n",
    "def placeholder_inputs(batch_size, num_point):\n",
    "    pointclouds_pl = tf.placeholder(tf.float32, shape=(batch_size, num_point, 8))\n",
    "    labels_pl = tf.placeholder(tf.int32, shape=(batch_size, num_point, 3))\n",
    "    return pointclouds_pl, labels_pl\n",
    "\n",
    "\n",
    "def get_model(point_cloud, is_training, bn_decay=None):\n",
    "    \"\"\" Classification PointNet, input is BxNx3, output Bx40 \"\"\"\n",
    "    batch_size = point_cloud.get_shape()[0].value\n",
    "    num_point = point_cloud.get_shape()[1].value\n",
    "    end_points = {}\n",
    "    # The first transform is for input\n",
    "    with tf.variable_scope('transform_net1') as sc:\n",
    "        transform = input_transform_net(point_cloud, is_training, bn_decay, K=10) # output dimentions after fransform\n",
    "    point_cloud_transformed = tf.matmul(point_cloud, transform)\n",
    "    # BxNx10\n",
    "    input_image = tf.expand_dims(point_cloud_transformed, -1)\n",
    "    # BxNx10x1\n",
    "    # the second parameter is for the output number of feature maps(FM) not the size of FM\n",
    "    net = tf_util.conv2d(input_image, 64, [1,10],\n",
    "                         padding='VALID', stride=[1,1],\n",
    "                         bn=True, is_training=is_training,\n",
    "                         scope='conv1', bn_decay=bn_decay)\n",
    "    # BxNx1x64 ,in other words,the size of FM is Nx1 and 64 FMs\n",
    "    net = tf_util.conv2d(net, 64, [1,1],\n",
    "                         padding='VALID', stride=[1,1],\n",
    "                         bn=True, is_training=is_training,\n",
    "                         scope='conv2', bn_decay=bn_decay)\n",
    "    # BxNx1x64\n",
    "    with tf.variable_scope('transform_net2') as sc:\n",
    "        transform = feature_transform_net(net, is_training, bn_decay, K=64)\n",
    "    end_points['transform'] = transform\n",
    "    # after squeeze BxNx64 ,and we know the shape of transform is Bx64x64, so we get the feature transform result BxNx64\n",
    "    net_transformed = tf.matmul(tf.squeeze(net, axis=[2]), transform) \n",
    "    # BxNx64\n",
    "    net_transformed = tf.expand_dims(net_transformed, [2])\n",
    "    \n",
    "    point_feat = net_transformed # TODO record point-wise feature\n",
    "    # BxNx1x64\n",
    "    net = tf_util.conv2d(net_transformed, 64, [1,1],\n",
    "                         padding='VALID', stride=[1,1],\n",
    "                         bn=True, is_training=is_training,\n",
    "                         scope='conv3', bn_decay=bn_decay)\n",
    "    # BxNx1x64\n",
    "    net = tf_util.conv2d(net, 128, [1,1],\n",
    "                         padding='VALID', stride=[1,1],\n",
    "                         bn=True, is_training=is_training,\n",
    "                         scope='conv4', bn_decay=bn_decay)\n",
    "    # BxNx1x128\n",
    "    net = tf_util.conv2d(net, 1024, [1,1],\n",
    "                         padding='VALID', stride=[1,1],\n",
    "                         bn=True, is_training=is_training,\n",
    "                         scope='conv5', bn_decay=bn_decay)\n",
    "    # BxNx1x1024\n",
    "\n",
    "    # Symmetric function: max pooling\n",
    "    net = tf_util.max_pool2d(net, [num_point,1],\n",
    "                             padding='VALID', scope='maxpool')\n",
    "\n",
    "    # Bx1x1x1024  Global Feature TODO the dimentions of GF\n",
    "    global_feat = net\n",
    "    global_feat_expand = tf.tile(global_feat, [1, num_point, 1, 1])\n",
    "    print(global_feat_expand)\n",
    "    print(point_feat)\n",
    "    concat_feat = tf.concat([point_feat, global_feat_expand], 3)\n",
    "    \n",
    "#     net = tf.reshape(net, [batch_size, -1])\n",
    "#     # Bx1024\n",
    "#     net = tf_util.fully_connected(net, 512, bn=True, is_training=is_training,\n",
    "#                                   scope='fc1', bn_decay=bn_decay)\n",
    "#     net = tf_util.dropout(net, keep_prob=0.7, is_training=is_training,\n",
    "#                           scope='dp1')\n",
    "#     net = tf_util.fully_connected(net, 256, bn=True, is_training=is_training,\n",
    "#                                   scope='fc2', bn_decay=bn_decay)\n",
    "#     net = tf_util.dropout(net, keep_prob=0.7, is_training=is_training,\n",
    "#                           scope='dp2')\n",
    "#     net = tf_util.fully_connected(net, 40, activation_fn=None, scope='fc3')\n",
    "    # input BxNx1x(64+1024)\n",
    "    net = tf_util.conv2d(concat_feat, 512, [1,1],\n",
    "                         padding='VALID', stride=[1,1],\n",
    "                         bn=True, is_training=is_training,\n",
    "                         scope='conv6', bn_decay=bn_decay)\n",
    "    net = tf_util.conv2d(net, 256, [1,1],\n",
    "                         padding='VALID', stride=[1,1],\n",
    "                         bn=True, is_training=is_training,\n",
    "                         scope='conv7', bn_decay=bn_decay)\n",
    "    net = tf_util.conv2d(net, 128, [1,1],\n",
    "                         padding='VALID', stride=[1,1],\n",
    "                         bn=True, is_training=is_training,\n",
    "                         scope='conv8', bn_decay=bn_decay)\n",
    "    net = tf_util.conv2d(net, 32, [1,1],\n",
    "                         padding='VALID', stride=[1,1],\n",
    "                         bn=True, is_training=is_training,\n",
    "                         scope='conv9', bn_decay=bn_decay)\n",
    "\n",
    "    net = tf_util.conv2d(net, 3, [1,1],\n",
    "                         padding='VALID', stride=[1,1], activation_fn=None,\n",
    "                         scope='conv10')\n",
    "    print(net.shape)\n",
    "    net = tf.squeeze(net, [2]) # BxNxC\n",
    "    print(net.shape)\n",
    "    return net, end_points\n",
    "\n",
    "\n",
    "def get_loss(pred, label, end_points, reg_weight=0.001):\n",
    "    \"\"\" pred: B*NUM_CLASSES\n",
    "        label: B,\n",
    "        for fluid pred BxNx3,label BxNx3\n",
    "    \"\"\"\n",
    "    loss = tf.losses.mean_squared_error(label, pred)\n",
    "    regression_loss = tf.reduce_sum(loss)\n",
    "    print(regression_loss)\n",
    "    tf.summary.scalar('classify loss', regression_loss)\n",
    "#     loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=pred, labels=label)\n",
    "#     classify_loss = tf.reduce_mean(loss)\n",
    "#     tf.summary.scalar('classify loss', classify_loss)\n",
    "\n",
    "    # Enforce the transformation as orthogonal matrix\n",
    "    transform = end_points['transform'] # BxKxK\n",
    "    K = transform.get_shape()[1].value\n",
    "    mat_diff = tf.matmul(transform, tf.transpose(transform, perm=[0,2,1]))\n",
    "    mat_diff -= tf.constant(np.eye(K), dtype=tf.float32)\n",
    "    mat_diff_loss = tf.nn.l2_loss(mat_diff) \n",
    "    tf.summary.scalar('mat loss', mat_diff_loss)\n",
    "    print(type(mat_diff_loss))\n",
    "    return regression_loss + mat_diff_loss * reg_weight\n",
    "\n",
    "if __name__=='__main__':\n",
    "    with tf.Graph().as_default():\n",
    "        inputs = tf.zeros((32,1024,8))\n",
    "        outputs = get_model(inputs, tf.constant(True))\n",
    "        gvars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "        print(outputs)\n",
    "        init = tf.global_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init)\n",
    "            variables = {gvar.op.name: value for gvar, value in zip(gvars, sess.run(gvars))}\n",
    "            # print(variables)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transform_nets.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting models/transform_nets.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile models/transform_nets.py\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "sys.path.append(BASE_DIR)\n",
    "sys.path.append(os.path.join(BASE_DIR, '../utils'))\n",
    "import tf_util\n",
    "\n",
    "# K is define as the output dimention of transform net\n",
    "def input_transform_net(point_cloud, is_training, bn_decay=None, K=10):\n",
    "    \"\"\" Input (XYZ) Transform Net, input is BxNx8 gray image\n",
    "        Return:\n",
    "            Transformation matrix of size 3xK \n",
    "        specificly, input is 8, Transformation matrix is 8xK,K for output dimention\n",
    "    \"\"\"\n",
    "    batch_size = point_cloud.get_shape()[0].value\n",
    "    num_point = point_cloud.get_shape()[1].value\n",
    "    feature_dims = point_cloud.get_shape()[2].value # the basic infos of every particle are p_x,p_y,p_y,v_x,v_y,v_z,etc\n",
    "    input_image = tf.expand_dims(point_cloud, -1)\n",
    "    # BxNx8x1\n",
    "    # kernel size will determine the conv direct,(1,8) for vertical and (8,1)for horizon\n",
    "    net = tf_util.conv2d(input_image, 64, [1,feature_dims],\n",
    "                         padding='VALID', stride=[1,1],\n",
    "                         bn=True, is_training=is_training,\n",
    "                         scope='tconv1', bn_decay=bn_decay)\n",
    "    print(net.shape) # BxNx1x64\n",
    "    net = tf_util.conv2d(net, 128, [1,1],\n",
    "                         padding='VALID', stride=[1,1],\n",
    "                         bn=True, is_training=is_training,\n",
    "                         scope='tconv2', bn_decay=bn_decay)\n",
    "    print(net.shape) # BxNx1x128\n",
    "    net = tf_util.conv2d(net, 1024, [1,1],\n",
    "                         padding='VALID', stride=[1,1],\n",
    "                         bn=True, is_training=is_training,\n",
    "                         scope='tconv3', bn_decay=bn_decay)\n",
    "    print(net.shape) # BxNx1x1024\n",
    "    net = tf_util.max_pool2d(net, [num_point,1],\n",
    "                             padding='VALID', scope='tmaxpool')\n",
    "    #(32, 1 , 1, 1024)\n",
    "    net = tf.reshape(net, [batch_size, -1])\n",
    "    net = tf_util.fully_connected(net, 512, bn=True, is_training=is_training,\n",
    "                                  scope='tfc1', bn_decay=bn_decay)\n",
    "    net = tf_util.fully_connected(net, 256, bn=True, is_training=is_training,\n",
    "                                  scope='tfc2', bn_decay=bn_decay)\n",
    "\n",
    "    with tf.variable_scope('transform_XYZ') as sc:\n",
    "#         assert(K==10)\n",
    "        weights = tf.get_variable('weights', [256, feature_dims*K],\n",
    "                                  initializer=tf.constant_initializer(0.0),\n",
    "                                  dtype=tf.float32)\n",
    "        biases = tf.get_variable('biases', [feature_dims*K],\n",
    "                                 initializer=tf.constant_initializer(0.0),\n",
    "                                 dtype=tf.float32)\n",
    "#         biases += tf.constant([1,0,0,0,1,0,0,0,1], dtype=tf.float32)\n",
    "        transform = tf.matmul(net, weights)\n",
    "        transform = tf.nn.bias_add(transform, biases)\n",
    "\n",
    "    transform = tf.reshape(transform, [batch_size, feature_dims, K])\n",
    "    print(transform.shape)# (32, 8, 10)\n",
    "    return transform\n",
    "\n",
    "\n",
    "def feature_transform_net(inputs, is_training, bn_decay=None, K=64):\n",
    "    \"\"\" Feature Transform Net, input is BxNx1xK\n",
    "        Return:\n",
    "            Transformation matrix of size KxK \"\"\"\n",
    "    batch_size = inputs.get_shape()[0].value\n",
    "    num_point = inputs.get_shape()[1].value\n",
    "    # BxNx1X64\n",
    "    net = tf_util.conv2d(inputs, 64, [1,1],\n",
    "                         padding='VALID', stride=[1,1],\n",
    "                         bn=True, is_training=is_training,\n",
    "                         scope='tconv1', bn_decay=bn_decay)\n",
    "    print(net.shape) # BxNx1x64\n",
    "    net = tf_util.conv2d(net, 128, [1,1],\n",
    "                         padding='VALID', stride=[1,1],\n",
    "                         bn=True, is_training=is_training,\n",
    "                         scope='tconv2', bn_decay=bn_decay)\n",
    "    print(net.shape) # BxNx1x128\n",
    "    net = tf_util.conv2d(net, 1024, [1,1],\n",
    "                         padding='VALID', stride=[1,1],\n",
    "                         bn=True, is_training=is_training,\n",
    "                         scope='tconv3', bn_decay=bn_decay)\n",
    "    # Bx1024x1x1024\n",
    "    net = tf_util.max_pool2d(net, [num_point,1],\n",
    "                             padding='VALID', scope='tmaxpool')\n",
    "    # Bx1x1x1024\n",
    "    net = tf.reshape(net, [batch_size, -1])\n",
    "    # Bx1024\n",
    "    net = tf_util.fully_connected(net, 512, bn=True, is_training=is_training,\n",
    "                                  scope='tfc1', bn_decay=bn_decay)\n",
    "    net = tf_util.fully_connected(net, 256, bn=True, is_training=is_training,\n",
    "                                  scope='tfc2', bn_decay=bn_decay)\n",
    "\n",
    "    with tf.variable_scope('transform_feat') as sc:\n",
    "        weights = tf.get_variable('weights', [256, K*K],\n",
    "                                  initializer=tf.constant_initializer(0.0),\n",
    "                                  dtype=tf.float32)\n",
    "        biases = tf.get_variable('biases', [K*K],\n",
    "                                 initializer=tf.constant_initializer(0.0),\n",
    "                                 dtype=tf.float32)\n",
    "        biases += tf.constant(np.eye(K).flatten(), dtype=tf.float32)\n",
    "        transform = tf.matmul(net, weights)\n",
    "        transform = tf.nn.bias_add(transform, biases)\n",
    "\n",
    "    transform = tf.reshape(transform, [batch_size, K, K])\n",
    "    # Bx64x64\n",
    "    return transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model_utils.py\n",
    "import os, argparse\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# The original freeze_graph function\n",
    "# from tensorflow.python.tools.freeze_graph import freeze_graph \n",
    "\n",
    "dir = os.path.dirname(os.path.realpath(__file__))\n",
    "\n",
    "def freeze_graph(model_dir, output_node_names):\n",
    "    \"\"\"Extract the sub graph defined by the output nodes and convert \n",
    "    all its variables into constant \n",
    "    Args:\n",
    "        model_dir: the root folder containing the checkpoint state file\n",
    "        output_node_names: a string, containing all the output node's names, \n",
    "                            comma separated\n",
    "    \"\"\"\n",
    "    if not tf.gfile.Exists(model_dir):\n",
    "        raise AssertionError(\n",
    "            \"Export directory doesn't exists. Please specify an export \"\n",
    "            \"directory: %s\" % model_dir)\n",
    "\n",
    "    if not output_node_names:\n",
    "        print(\"You need to supply the name of a node to --output_node_names.\")\n",
    "        return -1\n",
    "\n",
    "    # We retrieve our checkpoint fullpath\n",
    "    checkpoint = tf.train.get_checkpoint_state(model_dir)\n",
    "    input_checkpoint = checkpoint.model_checkpoint_path\n",
    "    \n",
    "    # We precise the file fullname of our freezed graph\n",
    "    absolute_model_dir = \"/\".join(input_checkpoint.split('/')[:-1])\n",
    "    output_graph = absolute_model_dir + \"/frozen_model.pb\"\n",
    "\n",
    "    # We clear devices to allow TensorFlow to control on which device it will load operations\n",
    "    clear_devices = True\n",
    "\n",
    "    # We start a session using a temporary fresh Graph\n",
    "    with tf.Session(graph=tf.Graph()) as sess:\n",
    "        # We import the meta graph in the current default Graph\n",
    "        saver = tf.train.import_meta_graph(input_checkpoint + '.meta', clear_devices=clear_devices)\n",
    "\n",
    "        # We restore the weights\n",
    "        saver.restore(sess, input_checkpoint)\n",
    "\n",
    "        # We use a built-in TF helper to export variables to constants\n",
    "        output_graph_def = tf.graph_util.convert_variables_to_constants(\n",
    "            sess, # The session is used to retrieve the weights\n",
    "            tf.get_default_graph().as_graph_def(), # The graph_def is used to retrieve the nodes \n",
    "            output_node_names.split(\",\") # The output node names are used to select the usefull nodes\n",
    "        ) \n",
    "\n",
    "        # Finally we serialize and dump the output graph to the filesystem\n",
    "        with tf.gfile.GFile(output_graph, \"wb\") as f:\n",
    "            f.write(output_graph_def.SerializeToString())\n",
    "        print(\"%d ops in the final graph.\" % len(output_graph_def.node))\n",
    "\n",
    "    return output_graph_def\n",
    "\n",
    "def load_graph(frozen_graph_filename):\n",
    "    # We load the protobuf file from the disk and parse it to retrieve the \n",
    "    # unserialized graph_def\n",
    "    with tf.gfile.GFile(frozen_graph_filename, \"rb\") as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "\n",
    "    # Then, we import the graph_def into a new Graph and returns it \n",
    "    with tf.Graph().as_default() as graph:\n",
    "        # The name var will prefix every op/nodes in your graph\n",
    "        # Since we load everything in a new graph, this is not needed\n",
    "        tf.import_graph_def(graph_def, name=\"prefix\")\n",
    "    return graph\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--model_dir\", type=str, default=\"log\", help=\"Model folder to export\")\n",
    "    parser.add_argument(\"--output_node_names\", type=str, default=\"pred\", help=\"The name of the output nodes, comma separated.\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    freeze_graph(args.model_dir, args.output_node_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run -i models/pointnet_cls.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "If you have trouble in understanding weights of conv layers, read this sampling.\n",
    "\"\"\"\n",
    "channels = 3\n",
    "filters_test = np.zeros(shape=(7, 7, channels, 2), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters_test[:,3,:,0] = 1\n",
    "filters_test[3,:,:,1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filters_test[3,:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filters_test[:,3,:,0] = 0\n",
    "filters_test[3,:,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.]],\n",
       "\n",
       "       [[1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.]],\n",
       "\n",
       "       [[1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.]],\n",
       "\n",
       "       [[1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.]],\n",
       "\n",
       "       [[1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.]],\n",
       "\n",
       "       [[1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.]],\n",
       "\n",
       "       [[1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.]]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filters_test[:, 3, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[[1, 2, 3],[4, 5, 6]],[[2, 4, 6],[9, 5, 6]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channels = 3\n",
    "filters_test = np.zeros(shape=(7, 7, channels, 2), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.array([1 , 3])\n",
    "d = np.array([2 , 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(c, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = a-b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-1, -1, -1],\n",
       "        [-1, -1, -1]],\n",
       "\n",
       "       [[-1, -1, -1],\n",
       "        [-1, -1, -1]]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = a.shape[0]\n",
    "count = 0\n",
    "for i in range(batch_size):\n",
    "    count += np.reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.sum(e*e, 2),0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n",
      "6.0\n",
      "30.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    a = tf.Variable(5.0, name='a')\n",
    "    b = tf.Variable(6.0, name='b')\n",
    "    c = tf.multiply(a, b, name=\"c\")\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    print a.eval() # 5.0\n",
    "    print b.eval() # 6.0\n",
    "    print c.eval() # 30.0\n",
    "    \n",
    "    tf.train.write_graph(sess.graph_def, 'models/', 'graph.pb', as_text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.multiply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 2,  6, 12],\n",
       "        [20, 30, 42]],\n",
       "\n",
       "       [[ 6, 20, 42],\n",
       "        [90, 30, 42]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.multiply(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1, 2, 3],\n",
       "        [4, 5, 6]],\n",
       "\n",
       "       [[2, 4, 6],\n",
       "        [9, 5, 6]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channels = 3\n",
    "filters_test = np.zeros(shape=(7, 7, channels, 2), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'reduce_max'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-eebefc5500e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_max\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'module' object has no attribute 'reduce_max'"
     ]
    }
   ],
   "source": [
    "np.reduce_max(a, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 12440206949566615293]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "\n",
    "get_available_gpus()\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1, 2, 3],\n",
       "        [4, 5, 6]]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[[True,False]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
