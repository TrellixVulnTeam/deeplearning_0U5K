codejam

比实现代码更重要的是思考问题的方式以及如何构思将要采取的方案。

明确的知道你基本上可以解决问题比一昧的担心问题的一些细枝末节而迟迟不动手要有用的多。

发现问题的模式和策略

- [ ] 树
- [ ] 图
      - [ ] 存储结构
- [ ] 查找
- [ ] 排序

## 深度概率

以某种合适的方式去构建一种妥当的近似模式。



**基本概念**（常用到但会混淆的定义）

有向图模型是结构化概率模型，也称信念网络或者贝叶斯网络

无向模型被称为马尔可夫随机场或者马尔可夫网络。

大数定理：大数定律又称大数法则、大数律，是描述相当多次数重复实验的结果的定律。根据这个定律知道，样本数量越多，则其平均就越趋近期望值。

中心极限定理：中心极限定理说明，在适当的条件下，大量相互独立随机变量的均值经适当标准化后依分布收敛于正态分布。

函数的加权平均值$E[f]=\sum_x p(x)f(x)$，某个函数$f(x)$在概率分布p(x)下的均值称为$f(x)$的期望。



bias function 数据预处理，归一化一类

概率与频率视角的差异在于你最终得到的是关于t的点估计还是t的概率分布

### 概率论视角

> 概率用于在已知一些参数的情况下，预测接下来的观测所得到的结果；而似然性则是用在已知某些观测所得到的结果时，对有关事物的性质的**参数**进行估计。

$$
p(\vec w|D) = \cfrac{p(D|\vec w)p(\vec w)}{p(D)}
$$

> 通常情况下，会按照某个分布$p(\vec w)$对参数$\vec w$进行初始化,$p(\vec w)$称为$\vec w$的先验分布，而我们的目标是要得到$\vec w$的后验概率分布$p(\vec w|D)$，即观测到数据D后$\vec w$的分布。
>
> 
>
> 再来Baye定理的右边，$p(D|\vec w)$是似然函数，是关于参数$\vec w$的函数，表达已知有事件D发生，运用似然函数$L(\vec w|D)$，估计参数$\vec w$的可能性	，似然函数不满足归一性，似然性不等同于概率,但它是$\vec w = \vec w_i$时的条件概率，$p(D|\vec w)$表达的意义是，在观测到D时，$\vec w = \vec w_i$的似然性（是似然性而不是概率，似然函数不满足归一性，重要性质重复说多少遍都不为过），如果此时$\vec w = \vec w_j(j\ne i)$,那么似然函数的值也会改变，**似然函数的重要性不是它的具体取值，而是当参数变化时函数到底变小还是变大**。自然的，引出极大似然估计：似然函数取得最大时表示相应的参数能够使得统计模型最为合理。
>
> PS：我觉得似然函数的值不是概率的另一个解释可以是，能不能称作概率取决于你当前所处的阶段，如果你是在基于观测数据D进行$\vec w$参数的估计，那么此时只表达似然函数表达的是似然性而不是条件概率，而如果你是基于$\vec w = \vec w_i$的前提，去预测接下来将会出现的数据，那么此时似然函数那个表达式（这个时候也就不应当称作似然函数了，虽然还是同一个表达式）所计算的就是条件概率。这其实就回到了概率和似然性的定义了。
>
> 
>
> 在每一轮的训练过程中，我们都可以假设在当前时刻下$\vec w = \vec w_i$是已知的，已知参数$\vec w$的情况下，D发生的概率为$p(D|\vec w) = \cfrac{p(\vec w,D)}{p(\vec w)}$，当然我们不这么计算似然函数，而是以此计算$p(\vec w,D)$.
>
> 
>
> 根据Beya定理的加法原则，上式的分母$p(D)$可以表达为：
> $$
> p(D)=\int p(D|\vec w)p(\vec w)d\vec w
> $$
>

### 概率论解释机器学习

有了上面的基础，下面我们讨论一个本质的问题：机器学习问题最终都会转化为解目标函数的优化问题，那么我们有没有想过这个目标函数是如何生成的。

> MLE（Maximam Likelihood Estimation,极大似然估计）和MAP（Maximam a Posterior最大后验概率估计）是生成目标函数的很基本思想，我们需要对其有着深刻理解。

- [ ] 我们又有没有考虑过这样一些问题，为什么将均方差（MSE）和交叉熵损失分别作为回归和分类任务的目标函数？为什么增加一个正则项是有意义的？
      - [ ] 分类任务的交叉损失熵是如何与MLE相关联的？

            - [x] 交叉熵是啥？
            - [x] softmax是啥？
            - [x] KL divergence是啥？

      - [ ] 均方误差呢？

      - [ ] 正则化项的意义是什么？

            MLE中添加的L2正则化项等价于MAP中使用高斯分布作为先验概率分布

#### MLE和MAP

**针锋相对**：

不管是贝叶斯学派还是频率学派，似然函数都发挥着核心作用，但是二者的理解方式还是有着很大的不同。两者最本质的区别是看待世界的视角不同，频率学派认为与事物有关的参数是客观存在的，不会改变，虽然未知，但是是固定值；贝叶斯学派认为参数是随机的，首先有一个先验概率分布，然后需要根据事实（观测数据）去不断调整参数的概率分布，以找到能够对现实进行最优描述的概率分布。

MLE在小样本上偏差大，MAP先验不同给出的结果不同

#### 理解目标函数

首先我们来看经常作为分类和回归任务目标函数的交叉熵损失和均方误差：
$$
CE = -\sum_x p(x)logq(x)
$$
其中，$p(x)$是正确的标注，$q(x)$是网络输出
$$
MSE = \cfrac1N\sum_{i=1}^N (\hat y_i - y_i)^2
$$
其中y是正确的标注，$\hat y$是网络输出



##### 分类

二分类激活函数使用sigmoid(将输入映射到[0, 1]区间)，多分类激活函数使用softmax(对输出值归一化为概率值)

sigmoid:
$$
Sigmoid(x)={\frac {1}{1+e^{-wx}}}={\frac {e^{wx}}{e^{wx}+1}}
$$
![/images_md/sigmoid.png](/images_md/sigmoid.png)
softmax:
$$
\sigma :\mathbb {R} ^{K}\to \left\{z\in \mathbb {R} ^{K}|z_{i}>0,\sum _{i=1}^{K}z_{i}=1\right\}
$$

$$
\sigma (\mathbf {z} )_{j}={\frac {e^{z_{j}}}{\sum _{k=1}^{K}e^{z_{k}}}}  for j = 1, …, K.
$$
$$
{\displaystyle P(y=j\mid \mathbf {x} )={\frac {e^{\mathbf {x} ^{\mathsf {T}}\mathbf {w} _{j}}}{\sum _{k=1}^{K}e^{\mathbf {x} ^{\mathsf {T}}\mathbf {w} _{k}}}}}
$$

其实分类的损失函数和你分类正确与否没有直接关系，而是基于观察数据，在极大似然函数里加入标签对应类别的概率，不管是二分类还是多分类，最终正确的分类标签只有一个，在进行似然估计时也仅考虑观测数据相关

对于二分类问题：

- [x] 对于输入x,类标签是t，我们的目标是找到$\vec w,y=sigmoid(\vec w \vec x)$使得$p(t|x)$最大:$p(t|x) = (y)^t(1-y)^{1-t}$,因为sigmoid函数默认输出的就是t=1时的概率，在进行极大似然估计时，会选择和观测数据的类别所对应的概率加入到似然函数中，对于所有的观测样本，似然函数可以表达为:
      $$
      L_{ce} = \prod_{i=1}^Np(t_i|x_i)=\prod_{i=1}^N(y_i)^{t_i}(1-y_i)^{1-t_i}
      $$


对于多分类问题，其中假定C是分类类别，$\vec t$为ont-hot向量，每个类别的输出概率计为$y_i=P(t_i|x)$：
$$
p(\vec t|x) =\prod_{i=1}^C (y_i)^{t_i}
$$

$$
Like = \prod_{i=1}^Np(\vec t_i|x)
$$

转化为最小化负对数似然函数
$$
-logp(t|x) =-\sum_{i=1}^C{t_i}log (y_i)
$$

$$
-logLike = -\sum_{i=1}^Nln\ p(t|x)
$$



**交叉熵**

相对熵定义为，**衡量使用$q(x)$近似$p(x)$时的信息损失**：
$$
\begin{split}D_{KL}(p||q) &= \sum_{x \in X} p(x) log \frac{p(x)}{q(x)} \\& =\sum_{x \in X}p(x)log \ p(x) - \sum_{x \in X}p(x)log \ q(x) \\& =-H(p) - \sum_{x \in X}p(x)log\ q(x)\end{split}
$$

交叉熵定义为：
$$
CE(p, q) = -\sum_{x \in X}p(x)log\ q(x) = H(p) + D_{KL}(p||q)
$$



根据交叉熵的定义，我们可以得到二分类和多分类问题的交叉熵损失函数分别为：
$$
l_{bi}=-\sum_{i=1}^N (t_ilog\ y_i+(1-t_i)log\ (1-y_i))
$$

$$
l_{multi}=-\sum_{i=1}^N\sum_{j=1}^Ct_{ij}log (y_{ij})
$$

综上，分类问题的交叉熵损失函数与极小化负对数似然函数形式是一样的。

- [ ] 即便我们知道了这二者形式一样，也可以根据MLE去理解交叉熵损失函数的意义，但是为什么依据交叉熵定义写出的损失函数会和MLE的形式一样呢，交叉熵本身的定义里和极大似然函数是否有什么关系呢？

##### 回归

在深入回归问题的目标函数之前，我们先来看这样一张图：


![/images_md/awesome.png](/images_md/awesome.png)

这张图是学习机器学习以来给予我最大启发的一张图，那么我们应该如何理解这张图所要传达给我们的信息呢？

> 首先我们基于这样一种假设，目标值t服从以$y(x,w)$为均值的高斯分布，因而有
> $$
> p(t|x,w,\beta)=N(t|y(x,w),\beta^{-1})
> $$
>

假设我们使用训练集$\{x,t\}$,如果假设数据间是独立，那么似然函数可以表达为：
$$
p(t|x,w,β) = \prod_{n=1}^NN( t_n|y(x_n, w), β^{−1})
$$

$$
-lnp(t|x,w,β)= -\cfrac{\beta}2\sum_{n-1}^N \{y(x_n,w)−t_n\}^2 + \cfrac N2 lnβ−\cfrac N 2 ln(2π).
$$

均方误差与假设t服从高斯分布并使用该分布对观测数据进行极大似然估计是一致的！换言之极大似然估计求的的参数值可以由使用反向传播的随机梯度下降算法优化均方误差得到

因而对于模型输出的每一个预测值t，t取高斯分布最大概率时取值，即高斯分布的均值处，我们的目标就是基于观测数据来找到使似然函数取得最大值时的参数$\vec w*$，这也正是均方误差的意义所在。

- [ ] 那么均方误差的提出是不是独立的呢？是先有的均方误差后来才碰巧发现了其所包含的假设和意义吗？

正则化项的意义则可以由最大后验概率估计来解释，L2正则化项可以由基于正态分布的先验假设来解释。
$$
p(w|x, t, α, β) \varpropto p(t|x, w, β)p(w|α).
$$





### 变分推理

通过寻求最可能接近真实分布的近似分布$q(h|v)$来逼近真实分布$p(h|v)$



许多概率模型由未归一化的概率分布$\hat p(x;\theta)$定义，必须通过除以配分函数来归一化$\hat p$,以获得一个有效的概率分布。

> 配分函数：是未归一化概率所有状态的积分$\int\hat p(x)dx$（连续变量）或求和$\sum_x\hat p(x)$（离散变量）,而对于很多有趣的模型来说，以上积分或求和难以计算。

通过最大似然学习无向模型特别困难的原因在于配分函数的计算依赖于参数。

#### MCMC

> 构建一个收敛到目标分布的估计序列

当无法精确计算和或者积分（例如，和具有指数数量个项，且无法被精确简化）时，通常可以使用MCMC采样来近似它。这种想法把和或者积分视作某分布下的期望，然后通过估计对应的平均值来近似这个期望。令$s=\sum_x p(x)f(x)=E_p[f(x)]$或者$s=\int p(x)f(x)dx=E_p[f(x)]$为我们需要估计的和或者积分，写成期望的形式，p是一个关于随机变量x的概率分布（求和时）或者概率密度函数（求积分时）。

两个阶段

1. 磨合过程，从运行马尔可夫链开始到其达到均衡分布的过程
2. 从均衡分布中抽取样本序列



通常无法通过表达状态序列，转移矩阵，转移矩阵特征值来判断马尔可夫链是否已经混合成功，只能运行一段足够长的时间并通过启发式的方法判断是否混合成功（包括手动检查样本或者衡量前后样本间的相关性）。

- [ ] MCMC只适用于小规模问题，variational Bayes and expectation propagation的出现使得MCMC可以应用于较大规模问题

收敛：

MCMC的理论可以证明，经过一定次数的迭代之后，本方法一定会收敛的。在一定的迭代次数后所得到的稳定分布会十分接近目标寻求的联合分布。

Burn-in：很明显的最初的一些迭代得到的分布会和目标的后验分布差距很远，因而前N轮的迭代基本是可以直接剔除的。

##### 几种常用的MCMC采样方式

重要采样：

目前的理解是针对复杂的求和或者积分分解出恰当的px和fx，学习的最好的方式是有一个典型的重要采样的例子。



Gibbs采样：

Gibbs采样是最常用的MCMC采样方法

- [ ] 对每一个随机变量产生一个后验条件分布
- [ ] 从目标后验联合分布中模拟后验样本，对每一个随机变量从其它变量固定为当前值的后验条件分布中重复地采样

 Gibbs采样的局限性：

1. 即使得到完全的后验联合密度函数，很多情况下很是难以得到每一个随机变量的条件分布概率；
2. 再退一步，即使得到每一个变量的后验条件分布，可能也不是某种已知的分布形式，无法从中直接进行采样；
3. GIbbs采样可能不适合某些应用场景，效率非常低。




## 秣马厉兵

7. 假定图的邻接矩阵元素取值规则为：顶点之间存在边时对应元素值为1否则为0；下面关于图（顶点数大于1）的邻接矩阵描述正确的是

- 无向图的邻接矩阵对应特征值必然为非负数
- 无向图的邻接矩阵不一定可以对角化
- 有向无环图的邻接矩阵对应特征值必然为正数
- 无向完全图的邻接矩阵必然有大于0的特征值
- n顶点无向连通图对应邻接矩阵1的个数为2(n-1)则必然有环
- 以上描述都不正确

8. 对于矩阵的特征值和奇异值来说，以下说法正确的有
   - 奇异值分解只能作用于方针，而特征分解可以作用于任意矩阵
   - 特征分解只能作用于方阵，而奇艺值分解可以作用于任意矩阵
   - 奇异值分解和特征分解都可以作用于任意矩阵
   - 相比特征值，奇异值能更高效地压缩信息
   - 相比奇异值，特征值能更高效地压缩信息
   - 以上均不正确
9. 对数组[60,50,70,35,45,25]做从小到大排序（使用<堆排序>，建堆方式是从前往后建立最大堆例如[120，70，80，65]），从建堆到排序的过程中需要多少次元素交换？比如60，50，70，35，45，25->35,50,70,60,45,25,元素60和35交换记做一次元素交换

【4】二元关联的相关度矩阵，连通图的最大生成子树，该子树的关联度加和



- [ ] 机器学习理解目标函数及变分推理blog
- [ ] 图和树算法，邻接矩阵的性质